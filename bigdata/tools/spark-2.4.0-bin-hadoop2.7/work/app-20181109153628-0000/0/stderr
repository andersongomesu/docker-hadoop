Spark Executor Command: "/tools/jdk1.8.0_192/bin/java" "-cp" "$SPARK_HOME/lib/*:/tools/spark-2.4.0-bin-hadoop2.7/conf/:/tools/spark-2.4.0-bin-hadoop2.7/jars/*:/tools/hadoop-3.1.1/etc/hadoop/:/tools/hadoop-3.1.1/share/hadoop/common/lib/*:/tools/hadoop-3.1.1/share/hadoop/common/*:/tools/hadoop-3.1.1/share/hadoop/hdfs/:/tools/hadoop-3.1.1/share/hadoop/hdfs/lib/*:/tools/hadoop-3.1.1/share/hadoop/hdfs/*:/tools/hadoop-3.1.1/share/hadoop/mapreduce/lib/*:/tools/hadoop-3.1.1/share/hadoop/mapreduce/*:/tools/hadoop-3.1.1/share/hadoop/yarn/:/tools/hadoop-3.1.1/share/hadoop/yarn/lib/*:/tools/hadoop-3.1.1/share/hadoop/yarn/*:/tools/jdk1.8.0_192/lib/tools.jar" "-Xmx1024M" "-Dspark.driver.port=34267" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@hadoop-master:34267" "--executor-id" "0" "--hostname" "172.25.0.11" "--cores" "1" "--app-id" "app-20181109153628-0000" "--worker-url" "spark://Worker@172.25.0.11:37061"
========================================

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/tools/spark-2.4.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/tools/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
18/11/09 15:36:40 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 567@hadoop-slave1
18/11/09 15:36:40 INFO SignalUtils: Registered signal handler for TERM
18/11/09 15:36:40 INFO SignalUtils: Registered signal handler for HUP
18/11/09 15:36:40 INFO SignalUtils: Registered signal handler for INT
18/11/09 15:36:43 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
18/11/09 15:36:43 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
18/11/09 15:36:43 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
18/11/09 15:36:43 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
18/11/09 15:36:44 DEBUG Shell: setsid exited with exit code 0
18/11/09 15:36:44 DEBUG KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
18/11/09 15:36:44 DEBUG Groups:  Creating new Groups object
18/11/09 15:36:44 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
18/11/09 15:36:44 DEBUG NativeCodeLoader: Loaded the native-hadoop library
18/11/09 15:36:44 DEBUG JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
18/11/09 15:36:44 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
18/11/09 15:36:44 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
18/11/09 15:36:44 DEBUG SparkHadoopUtil: creating UGI for user: root
18/11/09 15:36:44 DEBUG UserGroupInformation: hadoop login
18/11/09 15:36:44 DEBUG UserGroupInformation: hadoop login commit
18/11/09 15:36:45 DEBUG UserGroupInformation: using local user:UnixPrincipal: root
18/11/09 15:36:45 DEBUG UserGroupInformation: Using user: "UnixPrincipal: root" with name root
18/11/09 15:36:45 DEBUG UserGroupInformation: User entry: "root"
18/11/09 15:36:45 DEBUG UserGroupInformation: UGI loginUser:root (auth:SIMPLE)
18/11/09 15:36:45 DEBUG UserGroupInformation: PrivilegedAction as:root (auth:SIMPLE) from:org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:64)
18/11/09 15:36:45 INFO SecurityManager: Changing view acls to: root
18/11/09 15:36:45 INFO SecurityManager: Changing modify acls to: root
18/11/09 15:36:45 INFO SecurityManager: Changing view acls groups to: 
18/11/09 15:36:45 INFO SecurityManager: Changing modify acls groups to: 
18/11/09 15:36:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
18/11/09 15:36:46 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
18/11/09 15:36:46 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
18/11/09 15:36:46 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
18/11/09 15:36:46 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 12
18/11/09 15:36:46 DEBUG PlatformDependent0: -Dio.netty.noUnsafe: false
18/11/09 15:36:46 DEBUG PlatformDependent0: Java version: 8
18/11/09 15:36:46 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
18/11/09 15:36:46 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
18/11/09 15:36:46 DEBUG PlatformDependent0: java.nio.Buffer.address: available
18/11/09 15:36:46 DEBUG PlatformDependent0: direct buffer constructor: available
18/11/09 15:36:46 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
18/11/09 15:36:46 DEBUG PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
18/11/09 15:36:46 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
18/11/09 15:36:46 DEBUG PlatformDependent: sun.misc.Unsafe: available
18/11/09 15:36:46 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
18/11/09 15:36:46 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
18/11/09 15:36:46 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
18/11/09 15:36:46 DEBUG PlatformDependent: -Dio.netty.maxDirectMemory: 954728448 bytes
18/11/09 15:36:46 DEBUG PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
18/11/09 15:36:46 DEBUG CleanerJava6: java.nio.ByteBuffer.cleaner(): available
18/11/09 15:36:46 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
18/11/09 15:36:46 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
18/11/09 15:36:46 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
18/11/09 15:36:46 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
18/11/09 15:36:46 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 9
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 9
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
18/11/09 15:36:46 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
18/11/09 15:36:47 DEBUG TransportClientFactory: Creating new connection to hadoop-master/172.25.0.10:34267
18/11/09 15:36:47 DEBUG DefaultChannelId: -Dio.netty.processId: 567 (auto-detected)
18/11/09 15:36:47 DEBUG NetUtil: -Djava.net.preferIPv4Stack: false
18/11/09 15:36:47 DEBUG NetUtil: -Djava.net.preferIPv6Addresses: false
18/11/09 15:36:47 DEBUG NetUtil: Loopback interface: lo (lo, 127.0.0.1)
18/11/09 15:36:47 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
18/11/09 15:36:47 DEBUG DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:19:00:0b (auto-detected)
18/11/09 15:36:47 DEBUG ByteBufUtil: -Dio.netty.allocator.type: pooled
18/11/09 15:36:47 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
18/11/09 15:36:47 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
18/11/09 15:36:47 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
18/11/09 15:36:47 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@40dfee48
18/11/09 15:36:47 DEBUG TransportClientFactory: Connection to hadoop-master/172.25.0.10:34267 successful, running bootstraps...
18/11/09 15:36:47 INFO TransportClientFactory: Successfully created connection to hadoop-master/172.25.0.10:34267 after 387 ms (0 ms spent in bootstraps)
18/11/09 15:36:47 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 32768
18/11/09 15:36:47 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
18/11/09 15:36:47 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
18/11/09 15:36:47 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
18/11/09 15:36:48 INFO SecurityManager: Changing view acls to: root
18/11/09 15:36:48 INFO SecurityManager: Changing modify acls to: root
18/11/09 15:36:48 INFO SecurityManager: Changing view acls groups to: 
18/11/09 15:36:48 INFO SecurityManager: Changing modify acls groups to: 
18/11/09 15:36:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
18/11/09 15:36:48 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
18/11/09 15:36:49 DEBUG TransportClientFactory: Creating new connection to hadoop-master/172.25.0.10:34267
18/11/09 15:36:49 DEBUG TransportClientFactory: Connection to hadoop-master/172.25.0.10:34267 successful, running bootstraps...
18/11/09 15:36:49 INFO TransportClientFactory: Successfully created connection to hadoop-master/172.25.0.10:34267 after 27 ms (0 ms spent in bootstraps)
18/11/09 15:36:49 INFO DiskBlockManager: Created local directory at /works/spark/spark-80b5f426-f59f-478b-bb65-3dbda5e8c6dd/executor-e9bb8017-7015-4ba6-ba8d-77c67325e00f/blockmgr-6255998e-7da6-4be6-8edb-7b7b1498a316
18/11/09 15:36:49 DEBUG DiskBlockManager: Adding shutdown hook
18/11/09 15:36:49 DEBUG ShutdownHookManager: Adding shutdown hook
18/11/09 15:36:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/11/09 15:36:50 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@hadoop-master:34267
18/11/09 15:36:50 INFO WorkerWatcher: Connecting to worker spark://Worker@172.25.0.11:37061
18/11/09 15:36:50 DEBUG TransportClientFactory: Creating new connection to /172.25.0.11:37061
18/11/09 15:36:50 DEBUG TransportClientFactory: Connection to /172.25.0.11:37061 successful, running bootstraps...
18/11/09 15:36:50 INFO TransportClientFactory: Successfully created connection to /172.25.0.11:37061 after 166 ms (0 ms spent in bootstraps)
18/11/09 15:36:50 INFO WorkerWatcher: Successfully connected to spark://Worker@172.25.0.11:37061
18/11/09 15:36:50 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
18/11/09 15:36:50 INFO Executor: Starting executor ID 0 on host 172.25.0.11
18/11/09 15:36:50 DEBUG TransportServer: Shuffle server started on port: 34177
18/11/09 15:36:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34177.
18/11/09 15:36:50 INFO NettyBlockTransferService: Server created on 172.25.0.11:34177
18/11/09 15:36:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/11/09 15:36:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.25.0.11, 34177, None)
18/11/09 15:36:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.25.0.11, 34177, None)
18/11/09 15:36:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.25.0.11, 34177, None)
18/11/09 15:36:51 INFO Executor: Using REPL class URI: spark://hadoop-master:34267/classes
