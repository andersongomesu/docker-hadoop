Spark Executor Command: "/tools/jdk1.8.0_192/bin/java" "-cp" "/tools/spark-2.4.0-bin-hadoop2.7/conf/:/tools/spark-2.4.0-bin-hadoop2.7/jars/*:/tools/hadoop-3.1.1/etc/hadoop/:/tools/hadoop-3.1.1/share/hadoop/common/lib/*:/tools/hadoop-3.1.1/share/hadoop/common/*:/tools/hadoop-3.1.1/share/hadoop/hdfs/:/tools/hadoop-3.1.1/share/hadoop/hdfs/lib/*:/tools/hadoop-3.1.1/share/hadoop/hdfs/*:/tools/hadoop-3.1.1/share/hadoop/mapreduce/lib/*:/tools/hadoop-3.1.1/share/hadoop/mapreduce/*:/tools/hadoop-3.1.1/share/hadoop/yarn/:/tools/hadoop-3.1.1/share/hadoop/yarn/lib/*:/tools/hadoop-3.1.1/share/hadoop/yarn/*:/tools/jdk1.8.0_192/lib/tools.jar" "-Xmx1024M" "-Dspark.driver.port=56891" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@thinkpad.localdomain:56891" "--executor-id" "0" "--hostname" "172.25.0.11" "--cores" "1" "--app-id" "app-20181109162444-0003" "--worker-url" "spark://Worker@172.25.0.11:37061"
========================================

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/tools/spark-2.4.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/tools/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
18/11/09 16:24:56 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 742@hadoop-slave1
18/11/09 16:24:56 INFO SignalUtils: Registered signal handler for TERM
18/11/09 16:24:56 INFO SignalUtils: Registered signal handler for HUP
18/11/09 16:24:56 INFO SignalUtils: Registered signal handler for INT
18/11/09 16:24:58 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
18/11/09 16:24:58 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
18/11/09 16:24:58 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
18/11/09 16:24:58 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
18/11/09 16:24:58 DEBUG Shell: setsid exited with exit code 0
18/11/09 16:24:58 DEBUG KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
18/11/09 16:24:58 DEBUG Groups:  Creating new Groups object
18/11/09 16:24:58 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
18/11/09 16:24:58 DEBUG NativeCodeLoader: Loaded the native-hadoop library
18/11/09 16:24:58 DEBUG JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
18/11/09 16:24:58 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
18/11/09 16:24:58 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
18/11/09 16:24:58 DEBUG SparkHadoopUtil: creating UGI for user: litianzhi
18/11/09 16:24:58 DEBUG UserGroupInformation: hadoop login
18/11/09 16:24:58 DEBUG UserGroupInformation: hadoop login commit
18/11/09 16:24:59 DEBUG UserGroupInformation: using local user:UnixPrincipal: root
18/11/09 16:24:59 DEBUG UserGroupInformation: Using user: "UnixPrincipal: root" with name root
18/11/09 16:24:59 DEBUG UserGroupInformation: User entry: "root"
18/11/09 16:24:59 DEBUG UserGroupInformation: UGI loginUser:root (auth:SIMPLE)
18/11/09 16:24:59 DEBUG UserGroupInformation: PrivilegedAction as:litianzhi (auth:SIMPLE) from:org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:64)
18/11/09 16:24:59 INFO SecurityManager: Changing view acls to: root,litianzhi
18/11/09 16:24:59 INFO SecurityManager: Changing modify acls to: root,litianzhi
18/11/09 16:24:59 INFO SecurityManager: Changing view acls groups to: 
18/11/09 16:24:59 INFO SecurityManager: Changing modify acls groups to: 
18/11/09 16:24:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, litianzhi); groups with view permissions: Set(); users  with modify permissions: Set(root, litianzhi); groups with modify permissions: Set()
18/11/09 16:24:59 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
18/11/09 16:24:59 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
18/11/09 16:24:59 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
18/11/09 16:24:59 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 12
18/11/09 16:24:59 DEBUG PlatformDependent0: -Dio.netty.noUnsafe: false
18/11/09 16:24:59 DEBUG PlatformDependent0: Java version: 8
18/11/09 16:24:59 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
18/11/09 16:24:59 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
18/11/09 16:24:59 DEBUG PlatformDependent0: java.nio.Buffer.address: available
18/11/09 16:24:59 DEBUG PlatformDependent0: direct buffer constructor: available
18/11/09 16:24:59 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
18/11/09 16:24:59 DEBUG PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
18/11/09 16:24:59 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
18/11/09 16:24:59 DEBUG PlatformDependent: sun.misc.Unsafe: available
18/11/09 16:24:59 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
18/11/09 16:24:59 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
18/11/09 16:24:59 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
18/11/09 16:24:59 DEBUG PlatformDependent: -Dio.netty.maxDirectMemory: 954728448 bytes
18/11/09 16:24:59 DEBUG PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
18/11/09 16:24:59 DEBUG CleanerJava6: java.nio.ByteBuffer.cleaner(): available
18/11/09 16:25:00 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
18/11/09 16:25:00 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
18/11/09 16:25:00 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
18/11/09 16:25:00 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
18/11/09 16:25:00 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 9
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 9
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
18/11/09 16:25:00 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
18/11/09 16:25:00 DEBUG TransportClientFactory: Creating new connection to thinkpad.localdomain/10.0.75.1:56891
18/11/09 16:25:00 DEBUG DefaultChannelId: -Dio.netty.processId: 742 (auto-detected)
18/11/09 16:25:00 DEBUG NetUtil: -Djava.net.preferIPv4Stack: false
18/11/09 16:25:00 DEBUG NetUtil: -Djava.net.preferIPv6Addresses: false
18/11/09 16:25:00 DEBUG NetUtil: Loopback interface: lo (lo, 127.0.0.1)
18/11/09 16:25:00 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
18/11/09 16:25:00 DEBUG DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:19:00:0b (auto-detected)
18/11/09 16:25:00 DEBUG ByteBufUtil: -Dio.netty.allocator.type: pooled
18/11/09 16:25:00 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
18/11/09 16:25:00 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
18/11/09 16:25:00 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
18/11/09 16:25:00 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@4493da60
18/11/09 16:25:00 DEBUG TransportClientFactory: Connection to thinkpad.localdomain/10.0.75.1:56891 successful, running bootstraps...
18/11/09 16:25:00 INFO TransportClientFactory: Successfully created connection to thinkpad.localdomain/10.0.75.1:56891 after 373 ms (0 ms spent in bootstraps)
18/11/09 16:25:00 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 32768
18/11/09 16:25:00 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
18/11/09 16:25:00 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
18/11/09 16:25:00 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
18/11/09 16:25:01 INFO SecurityManager: Changing view acls to: root,litianzhi
18/11/09 16:25:01 INFO SecurityManager: Changing modify acls to: root,litianzhi
18/11/09 16:25:01 INFO SecurityManager: Changing view acls groups to: 
18/11/09 16:25:01 INFO SecurityManager: Changing modify acls groups to: 
18/11/09 16:25:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, litianzhi); groups with view permissions: Set(); users  with modify permissions: Set(root, litianzhi); groups with modify permissions: Set()
18/11/09 16:25:01 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
18/11/09 16:25:02 DEBUG TransportClientFactory: Creating new connection to thinkpad.localdomain/10.0.75.1:56891
18/11/09 16:25:02 DEBUG TransportClientFactory: Connection to thinkpad.localdomain/10.0.75.1:56891 successful, running bootstraps...
18/11/09 16:25:02 INFO TransportClientFactory: Successfully created connection to thinkpad.localdomain/10.0.75.1:56891 after 12 ms (1 ms spent in bootstraps)
18/11/09 16:25:02 INFO DiskBlockManager: Created local directory at /works/spark/spark-80b5f426-f59f-478b-bb65-3dbda5e8c6dd/executor-658b3ce8-d9a0-4d09-acc8-4765b94aa03c/blockmgr-95785559-c30e-481c-b8f3-54fdc2febe32
18/11/09 16:25:02 DEBUG DiskBlockManager: Adding shutdown hook
18/11/09 16:25:02 DEBUG ShutdownHookManager: Adding shutdown hook
18/11/09 16:25:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/11/09 16:25:03 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@thinkpad.localdomain:56891
18/11/09 16:25:03 INFO WorkerWatcher: Connecting to worker spark://Worker@172.25.0.11:37061
18/11/09 16:25:03 DEBUG TransportClientFactory: Creating new connection to /172.25.0.11:37061
18/11/09 16:25:03 DEBUG TransportClientFactory: Connection to /172.25.0.11:37061 successful, running bootstraps...
18/11/09 16:25:03 INFO TransportClientFactory: Successfully created connection to /172.25.0.11:37061 after 61 ms (0 ms spent in bootstraps)
18/11/09 16:25:03 INFO WorkerWatcher: Successfully connected to spark://Worker@172.25.0.11:37061
18/11/09 16:25:03 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
18/11/09 16:25:03 INFO Executor: Starting executor ID 0 on host 172.25.0.11
18/11/09 16:25:04 DEBUG TransportServer: Shuffle server started on port: 44323
18/11/09 16:25:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44323.
18/11/09 16:25:04 INFO NettyBlockTransferService: Server created on 172.25.0.11:44323
18/11/09 16:25:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/11/09 16:25:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.25.0.11, 44323, None)
18/11/09 16:25:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.25.0.11, 44323, None)
18/11/09 16:25:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.25.0.11, 44323, None)
18/11/09 16:25:04 ERROR Inbox: Ignoring error
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readUTF(DataInputStream.java:609)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.spark.scheduler.TaskDescription$$anonfun$decode$1.apply(TaskDescription.scala:134)
	at org.apache.spark.scheduler.TaskDescription$$anonfun$decode$1.apply(TaskDescription.scala:133)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at org.apache.spark.scheduler.TaskDescription$.decode(TaskDescription.scala:133)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(CoarseGrainedExecutorBackend.scala:96)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
